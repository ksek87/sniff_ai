{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding4Fun\\sniff_ai\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import datasets\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import IntervalStrategy\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1c6815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu121\n",
      "CUDA Version: 12.1\n",
      "cuDNN Version: 90100\n",
      "Device: NVIDIA GeForce GTX 1650 SUPER\n",
      "CUDA Available: True\n",
      "Current CUDA Device: 0\n",
      "Device Count: 1\n",
      "Device Capability: (7, 5)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "print(f\"cuDNN Version: {torch.backends.cudnn.version()}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current CUDA Device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Device Capability: {torch.cuda.get_device_capability()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "load-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "dataset_path = os.path.join(project_root, 'data_collection', 'dataset.csv')\n",
    "df = pd.read_csv(dataset_path, encoding='utf-8-sig')\n",
    "dataset = Dataset.from_pandas(df)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "train_test_split_ratio = 0.1\n",
    "train_df, eval_df = train_test_split(df, test_size=train_test_split_ratio)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b486178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "      <th>Notes</th>\n",
       "      <th>Concepts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Costume National</td>\n",
       "      <td>SEA THRU</td>\n",
       "      <td>In Sea Thru, natural and molecular notes merge...</td>\n",
       "      <td>['Sea Water', 'Lemon', 'Oakmoss', 'Cypress', '...</td>\n",
       "      <td>['Dynamic', 'Sporty', 'Summer', 'Everyday', 'C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gritti</td>\n",
       "      <td>HYSTERICA</td>\n",
       "      <td>Hysterica, a life explosion, tailored for thos...</td>\n",
       "      <td>['Tuberose', 'Liquor', 'Plum', 'Gardenia', 'Pa...</td>\n",
       "      <td>['Everyday', 'Floral', 'Blooming', 'Round', 'R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4711</td>\n",
       "      <td>ORRIS &amp; SILK</td>\n",
       "      <td>A warm and floral scent with the powdery opule...</td>\n",
       "      <td>['Iris', 'Orris', 'Silk Tree Blossom. Discover...</td>\n",
       "      <td>['Everyday', 'Casual', 'Day', 'Romantic', 'Whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lomani</td>\n",
       "      <td>MADEMOISELLE</td>\n",
       "      <td>Mademoiselle is the portrait of an independent...</td>\n",
       "      <td>['Jasmine', 'Vanilla (Madagascar)', 'Red Berri...</td>\n",
       "      <td>['Round', 'Everyday', 'Rich', 'Intense', 'Robu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lomani</td>\n",
       "      <td>ELIXIR PERFECT</td>\n",
       "      <td>An elixir that brings unconditional cheerfulne...</td>\n",
       "      <td>['Narcissus', 'Cashmeran (Woody musky)', 'Almo...</td>\n",
       "      <td>['Everyday', 'Round', 'Rich', 'Casual', 'Inten...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Brand            Name  \\\n",
       "0  Costume National        SEA THRU   \n",
       "1            Gritti       HYSTERICA   \n",
       "2              4711    ORRIS & SILK   \n",
       "3            Lomani    MADEMOISELLE   \n",
       "4            Lomani  ELIXIR PERFECT   \n",
       "\n",
       "                                         Description  \\\n",
       "0  In Sea Thru, natural and molecular notes merge...   \n",
       "1  Hysterica, a life explosion, tailored for thos...   \n",
       "2  A warm and floral scent with the powdery opule...   \n",
       "3  Mademoiselle is the portrait of an independent...   \n",
       "4  An elixir that brings unconditional cheerfulne...   \n",
       "\n",
       "                                               Notes  \\\n",
       "0  ['Sea Water', 'Lemon', 'Oakmoss', 'Cypress', '...   \n",
       "1  ['Tuberose', 'Liquor', 'Plum', 'Gardenia', 'Pa...   \n",
       "2  ['Iris', 'Orris', 'Silk Tree Blossom. Discover...   \n",
       "3  ['Jasmine', 'Vanilla (Madagascar)', 'Red Berri...   \n",
       "4  ['Narcissus', 'Cashmeran (Woody musky)', 'Almo...   \n",
       "\n",
       "                                            Concepts  \n",
       "0  ['Dynamic', 'Sporty', 'Summer', 'Everyday', 'C...  \n",
       "1  ['Everyday', 'Floral', 'Blooming', 'Round', 'R...  \n",
       "2  ['Everyday', 'Casual', 'Day', 'Romantic', 'Whi...  \n",
       "3  ['Round', 'Everyday', 'Rich', 'Intense', 'Robu...  \n",
       "4  ['Everyday', 'Round', 'Rich', 'Casual', 'Inten...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e71c30c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU Device: NVIDIA GeForce GTX 1650 SUPER\n",
      "GPU Memory: 3.99957275390625 GB\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and setup device\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"GPU Memory:\", torch.cuda.get_device_properties(0).total_memory / 1024**3, \"GB\")\n",
    "    device = torch.device('cuda')\n",
    "    # Enable memory optimization\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"No GPU available, using CPU\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a59b5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding4Fun\\sniff_ai\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Keelin\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "d:\\Coding4Fun\\sniff_ai\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12217/12217 [00:16<00:00, 727.81 examples/s]\n",
      "Map: 100%|██████████| 1358/1358 [00:02<00:00, 654.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Initialize the tokenizer and set padding token\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')  # Use smaller GPT-2 tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "try:\n",
    "    model = GPT2LMHeadModel.from_pretrained('distilgpt2')  # Use smaller GPT-2 model\n",
    "    model = model.to(device)  # Explicitly move model to GPU\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"Model moved to: {next(model.parameters()).device}\")\n",
    "except RuntimeError as e:\n",
    "    if \"CUDA out of memory\" in str(e):\n",
    "        print(\"GPU out of memory. Try reducing batch size or model size\")\n",
    "        raise e\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    combined_text = []\n",
    "    for brand, name, desc, notes_str, concepts_str in zip(\n",
    "        examples['Brand'], \n",
    "        examples['Name'], \n",
    "        examples['Description'], \n",
    "        examples['Notes'], \n",
    "        examples['Concepts']\n",
    "    ):\n",
    "        try:\n",
    "            # Handle potential None values and parse lists\n",
    "            desc = str(desc) if desc is not None else \"\"\n",
    "            notes_str = str(notes_str) if notes_str is not None else \"[]\"\n",
    "            concepts_str = str(concepts_str) if concepts_str is not None else \"[]\"\n",
    "            \n",
    "            try:\n",
    "                notes_list = ast.literal_eval(notes_str)\n",
    "                concepts_list = ast.literal_eval(concepts_str)\n",
    "            except (ValueError, SyntaxError):\n",
    "                notes_list = []\n",
    "                concepts_list = []\n",
    "            \n",
    "            # Format the text in a structured way for the model to learn\n",
    "            text = f\"\"\"Description: {desc}\n",
    "            Notes: {', '.join(notes_list)}\n",
    "            Concepts: {', '.join(concepts_list)}\n",
    "            \"\"\"\n",
    "            combined_text.append(text)\n",
    "        except Exception as e:\n",
    "                print(f\"Error processing example: {e}\")\n",
    "                combined_text.append(\"\")  # Add empty string as fallback\n",
    "            \n",
    "    result = tokenizer(\n",
    "        combined_text,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=512,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Create labels for language modeling (shifted input_ids)\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Map the datasets with proper formatting\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    features=datasets.Features({\n",
    "        'input_ids': datasets.Sequence(datasets.Value('int32'), length=-1),\n",
    "        'attention_mask': datasets.Sequence(datasets.Value('int32'), length=-1),\n",
    "        'labels': datasets.Sequence(datasets.Value('int32'), length=-1)\n",
    "    })\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    features=datasets.Features({\n",
    "        'input_ids': datasets.Sequence(datasets.Value('int32'), length=-1),\n",
    "        'attention_mask': datasets.Sequence(datasets.Value('int32'), length=-1),\n",
    "        'labels': datasets.Sequence(datasets.Value('int32'), length=-1)\n",
    "    })\n",
    ")\n",
    "\n",
    "# Set format for PyTorch tensors\n",
    "tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "tokenized_eval.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0888e910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory Usage for each GPU:\n",
      "\n",
      "GPU 0: NVIDIA GeForce GTX 1650 SUPER\n",
      "Total Memory: 4095.56 MB\n",
      "Allocated Memory: 319.24 MB\n",
      "Cached Memory: 356.00 MB\n",
      "Free Memory: 3776.33 MB\n",
      "Memory Usage: 7.79%\n"
     ]
    }
   ],
   "source": [
    "# Get current CUDA memory usage\n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"\\nMemory Usage for each GPU:\")\n",
    "        for i in range(torch.cuda.device_count()):\n",
    "            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**2\n",
    "            allocated_memory = torch.cuda.memory_allocated(i) / 1024**2\n",
    "            cached_memory = torch.cuda.memory_reserved(i) / 1024**2\n",
    "            free_memory = total_memory - allocated_memory\n",
    "            \n",
    "            print(f\"\\nGPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "            print(f\"Total Memory: {total_memory:.2f} MB\")\n",
    "            print(f\"Allocated Memory: {allocated_memory:.2f} MB\")\n",
    "            print(f\"Cached Memory: {cached_memory:.2f} MB\")\n",
    "            print(f\"Free Memory: {free_memory:.2f} MB\")\n",
    "            print(f\"Memory Usage: {(allocated_memory/total_memory)*100:.2f}%\")\n",
    "    else:\n",
    "        print(\"No CUDA device available\")\n",
    "\n",
    "print_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fine-tune-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding4Fun\\sniff_ai\\venv\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  4%|▍         | 39/1000 [1:27:28<34:27:24, 129.08s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./sniff_model1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Coding4Fun\\sniff_ai\\venv\\lib\\site-packages\\transformers\\trainer.py:2164\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2162\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2165\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2169\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding4Fun\\sniff_ai\\venv\\lib\\site-packages\\transformers\\trainer.py:2527\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m   2522\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2525\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2526\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m-> 2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2528\u001b[0m ):\n\u001b[0;32m   2529\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2530\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fine-tune the model\n",
    "torch.cuda.empty_cache()\n",
    "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)  # Use smaller GPT-2 model\n",
    "model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # Reduce the number of epochs\n",
    "    per_device_train_batch_size=4,  # Reduce batch size\n",
    "    per_device_eval_batch_size=4,  # Reduce batch size\n",
    "    gradient_accumulation_steps=32,  # Increase gradient accumulation steps\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=IntervalStrategy.STEPS,\n",
    "    eval_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    dataloader_num_workers=4,  # Increase number of workers for data loading\n",
    "    prediction_loss_only=True,  # Only compute loss during evaluation\n",
    "    torch_empty_cache_steps=50,  # Clear CUDA cache every 50 steps\n",
    "    max_steps=1000  # Limit the number of training steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    ")\n",
    "\n",
    "# Clear CUDA cache before starting training\n",
    "torch.cuda.empty_cache()\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('./sniff_model1')\n",
    "tokenizer.save_pretrained('./sniff_model1')\n",
    "\n",
    "# Plot training loss\n",
    "training_loss = trainer.state.log_history\n",
    "steps = [log['step'] for log in training_loss if 'loss' in log]\n",
    "losses = [log['loss'] for log in training_loss if 'loss' in log]\n",
    "plt.plot(steps, losses, label='Training Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "training_loss = trainer.state.log_history\n",
    "steps = [log['step'] for log in training_loss if 'loss' in log]\n",
    "losses = [log['loss'] for log in training_loss if 'loss' in log]\n",
    "plt.plot(steps, losses, label='Training Loss')\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Time')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df54f9b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db8521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fragrance descriptions\n",
    "def generate_fragrance_description(prompt):\n",
    "    # Move tensors to device after tokenization\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(inputs['input_ids'], max_length=100, num_return_sequences=1)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "user_description = \n",
    "\"This fragrance reminds me a walk in the park on a sunny day, and playing chess with my granfather along the danish coastline.My grandfather is wearing his favorite cologne, and the scent of the ocean is in the air. The fragrance is fresh and clean, with a hint of saltiness. It is a comforting and nostalgic scent that brings back memories of my childhood.\"\n",
    "prompt = f\"given creative fragrance description: {user_description}, \n",
    "you must output a perfume that captures the essence of this description.\n",
    "You will output fragrance notes, concepts, sentiments, and the final fragrance description (Which outputs the user given description first).\n",
    "Please output the results in JSON format that can be parsed by the python program.\n",
    "\"\n",
    "print(generate_fragrance_description(prompt))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
